{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# background\n",
    "\n",
    "## KNN mutual information estimator method\n",
    "All estimators are computed using Algorithm 1 from [Kraskov](https://arxiv.org/abs/cond-mat/0305641), et al. In this method, local subspace densities are estimated by counting the number of nearest neighbors around each point. From this, the entropy is estimated and then the mutual information. In this package, we can compute the mutual information between the following pairs of random variables:\n",
    "\n",
    "* $Z = (X,Y)$ where $X$ and $Y$ are one-dimensional scalar random variables\n",
    "* $Z = (\\theta_1,\\theta_2)$ where $\\theta_1$ and $\\theta_2$ are one-dimensional angular random variables\n",
    "* $Z = (X,\\theta)$ where $X$ is a one-dimensional scalar random variable and $\\theta$ is a one-dimensional angular random variable\n",
    "* $Z = ((X_1,Y_1),(X_2,Y_2))$ where $(X_1, Y_1)$ and $(X_2,Y_2)$ are two-dimensional scalar random variables\n",
    "\n",
    "## time-lagged mutual information\n",
    "Time-lagged mutual information is an intermediate statistic between pure mutual information (time-independent) and transfer entropy (time-dependent).\n",
    "To compute time-lagged mutual information between two random time-dependent variables (or random process) or a time-dependent random variable and itself, we order both random variable or a random variable and a copy of itself. We sample the first variable and record the timestamps, then sample the second variable at the same timesteps plus the timelag. Compute mutual information using these two random variables and vary the timelag to determine significant timescales. \n",
    "* $Z(X,X + \\tau)$ or $Z(X,Y + \\tau)$ for a timelag of length $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import info_theory as it\n",
    "import numpy as np\n",
    "from scipy.special import digamma\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare Gaussian true mutual information to KNN-estimated mutual information, when the mean is 0 and the variance is unity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 10 #number of repetitions\n",
    "sample_size = 100 #sample size\n",
    "\n",
    "mu = [0,0] # expectation\n",
    "sigma = [[1,0.9],[0.9,1]] #covariance matrix\n",
    "\n",
    "MI_Gauss=[]\n",
    "\n",
    "for i in range(0, reps):\n",
    "\n",
    "    np.random.seed(None)\n",
    "    Gauss = np.random.multivariate_normal(mu, sigma, sample_size)\n",
    "    MI_Gauss = np.append(MI_Gauss, it.compute_MI_scalar(data = Gauss, K=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85564702, 0.98002917, 0.9707217 , 0.96236096, 1.07428578,\n",
       "       0.96342016, 0.76238563, 0.73054673, 1.1193387 , 0.91194191])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_Gauss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute true mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_MI_Gauss(sigma):\n",
    "    return(-0.5*np.log(1-sigma**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_MI = true_MI_Gauss(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02528142, 0.14966357, 0.1403561 , 0.13199535, 0.24392018,\n",
       "       0.13305456, 0.06797998, 0.09981887, 0.2889731 , 0.0815763 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = np.abs(true_MI- MI_Gauss)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notes\n",
    "You will find that the algorithm converges rapidly as sample size increases, with more reptitions. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e506e439b6c2a41c10c08ca866bcc31431389b80193f531684ca6672eba7259c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
